{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-02 03:01:12,164] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/haskari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Trainer, TrainingArguments, pipeline\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import multiprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_paraphrase/cnn.pkl', 'rb') as f:\n",
    "    cnn_chat=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11490"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cnn_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_chat[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n"
     ]
    }
   ],
   "source": [
    "#number of empty summaries\n",
    "count=0\n",
    "bad_index=[]\n",
    "for idx,item in enumerate(cnn_chat):\n",
    "    if item == []:\n",
    "        count+=1\n",
    "        bad_index.append(idx)\n",
    "print(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10939\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for items in cnn_chat:\n",
    "    # items=nltk.sent_tokenize(items)\n",
    "\n",
    "    if not items:\n",
    "        continue\n",
    "    elif len(items)==3:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11490\n"
     ]
    }
   ],
   "source": [
    "print(len(cnn_chat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "article_key = 'article'\n",
    "summary_key = 'highlights'\n",
    "\n",
    "data=data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object <genexpr> at 0x7f2996840900> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "data=data.select(i for i in range(len(data)) \n",
    "                 if i not in set(bad_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8d3daf79ec40a89b46c0396baf8cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/10947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example):\n",
    "\n",
    "    example[\"original_article\"] = example[article_key]\n",
    "    example[\"article\"] = nltk.sent_tokenize(example[article_key])\n",
    "    # if args.dataset == 'news':\n",
    "    # check=example[summary_key][0]['text']\n",
    "    # example[\"highlights\"] = nltk.sent_tokenize(check)\n",
    "    # else:\n",
    "    example[\"highlights\"] = nltk.sent_tokenize(example[summary_key])\n",
    "        \n",
    "    return example\n",
    "\n",
    "data = data.map(tokenize, num_proc=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41255\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in data['highlights']:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32827\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in cnn_chat:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_paraphrase/xsum.pkl', 'rb') as f:\n",
    "    xsum_chat=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "#number of empty summaries\n",
    "count=0\n",
    "bad_index=[]\n",
    "for idx,item in enumerate(xsum_chat):\n",
    "    if item == []:\n",
    "        count+=1\n",
    "        bad_index.append(idx)\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11237\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for items in xsum_chat:\n",
    "    #items=nltk.sent_tokenize(items)\n",
    "    if not items:\n",
    "        continue\n",
    "    elif len(items)==1:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"xsum\")\n",
    "article_key = 'document'\n",
    "summary_key = 'summary'\n",
    "\n",
    "data=data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.select(i for i in range(len(data)) \n",
    "                 if i not in set(bad_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1089a3aac4b24719ae53992fdc4015e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/11239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.map(tokenize, num_proc=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11240\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in data['highlights']:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11243\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in xsum_chat:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_paraphrase/news.pkl', 'rb') as f:\n",
    "    news_chat=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "#number of empty summaries\n",
    "count=0\n",
    "bad_index=[]\n",
    "for idx,item in enumerate(news_chat):\n",
    "    if item == []:\n",
    "        count+=1\n",
    "        bad_index.append(idx)\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for items in news_chat:\n",
    "    items=items\n",
    "    if not items:\n",
    "        continue\n",
    "    elif len(items)==1:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"argilla/news-summary\")\n",
    "article_key = 'text'\n",
    "summary_key = 'prediction'\n",
    "data = DatasetDict({\n",
    "    'train': data['test'],\n",
    "    'test': data['train']})\n",
    "\n",
    "data=data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object <genexpr> at 0x7fc3b65291c0> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "data=data.select(i for i in range(len(data)) \n",
    "                 if i not in set(bad_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_news(example):\n",
    "    example[\"original_article\"] = example[article_key]\n",
    "    example[\"article\"] = nltk.sent_tokenize(example[article_key])\n",
    "\n",
    "    check=example[summary_key][0]['text']\n",
    "    example[\"highlights\"] = nltk.sent_tokenize(check)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395a1468a7a841da82c817b93956c13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/993 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.map(tokenize_news, num_proc=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in data['highlights']:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in news_chat:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_paraphrase/reddit.pkl', 'rb') as f:\n",
    "    reddit_chat=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757\n"
     ]
    }
   ],
   "source": [
    "#number of empty summaries\n",
    "count=0\n",
    "bad_index=[]\n",
    "for idx,item in enumerate(reddit_chat):\n",
    "    if item == []:\n",
    "        count+=1\n",
    "        bad_index.append(idx)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3457\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for items in reddit_chat:\n",
    "    #items=nltk.sent_tokenize(items)\n",
    "    if not items:\n",
    "        continue\n",
    "    elif len(items)==1:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('reddit_tifu', 'long')\n",
    "article_key = 'documents'\n",
    "summary_key = 'tldr'\n",
    "    # 80% train, 20% test + validation\n",
    "train_testvalid = data['train'].train_test_split(test_size=0.2, seed=42)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "data = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "data=data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.select(i for i in range(len(data)) \n",
    "                 if i not in set(bad_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3e510ceabd43cb96d9fac748b727a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/3457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.map(tokenize, num_proc=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4789\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in data['highlights']:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3457\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in reddit_chat:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
