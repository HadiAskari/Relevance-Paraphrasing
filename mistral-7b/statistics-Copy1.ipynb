{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/haskari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Trainer, TrainingArguments, pipeline\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import multiprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/cnn.pkl', 'rb') as f:\n",
    "    cnn_chat=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11490"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cnn_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pence signed the Religious Freedom Restoration Act, which allows businesses to discriminate against gays and lesbians',\n",
       " 'This move has garnered Pence criticism from various quarters',\n",
       " 'The backlash from the LGBT community is expected to continue in the future']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_chat[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n"
     ]
    }
   ],
   "source": [
    "#number of empty summaries\n",
    "count=0\n",
    "bad_index=[]\n",
    "for idx,item in enumerate(cnn_chat):\n",
    "    if item == []:\n",
    "        count+=1\n",
    "        bad_index.append(idx)\n",
    "print(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8070\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for items in cnn_chat:\n",
    "    # items=nltk.sent_tokenize(items)\n",
    "\n",
    "    if not items:\n",
    "        continue\n",
    "    elif len(items)==3:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11490\n"
     ]
    }
   ],
   "source": [
    "print(len(cnn_chat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "article_key = 'article'\n",
    "summary_key = 'highlights'\n",
    "\n",
    "data=data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.select(i for i in range(len(data)) \n",
    "                 if i not in set(bad_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=48): 100%|████████| 11204/11204 [00:00<00:00, 14675.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(example):\n",
    "\n",
    "    example[\"original_article\"] = example[article_key]\n",
    "    example[\"article\"] = nltk.sent_tokenize(example[article_key])\n",
    "    # if args.dataset == 'news':\n",
    "    # check=example[summary_key][0]['text']\n",
    "    # example[\"highlights\"] = nltk.sent_tokenize(check)\n",
    "    # else:\n",
    "    example[\"highlights\"] = nltk.sent_tokenize(example[summary_key])\n",
    "        \n",
    "    return example\n",
    "\n",
    "data = data.map(tokenize, num_proc=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42481\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in data['highlights']:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33884\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in cnn_chat:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/xsum_capped_random.pkl', 'rb') as f:\n",
    "    xsum_chat=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1363\n"
     ]
    }
   ],
   "source": [
    "#number of empty summaries\n",
    "count=0\n",
    "bad_index=[]\n",
    "for idx,item in enumerate(xsum_chat):\n",
    "    if item == []:\n",
    "        count+=1\n",
    "        bad_index.append(idx)\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9971\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for items in xsum_chat:\n",
    "    #items=nltk.sent_tokenize(items)\n",
    "    if not items:\n",
    "        continue\n",
    "    elif len(items)==1:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"xsum\")\n",
    "article_key = 'document'\n",
    "summary_key = 'summary'\n",
    "\n",
    "data=data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.select(i for i in range(len(data)) \n",
    "                 if i not in set(bad_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=48): 100%|██████████| 9971/9971 [00:00<00:00, 11872.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.map(tokenize, num_proc=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9971\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in data['highlights']:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9971\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in xsum_chat:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/news_capped_random.pkl', 'rb') as f:\n",
    "    news_chat=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#number of empty summaries\n",
    "count=0\n",
    "bad_index=[]\n",
    "for idx,item in enumerate(news_chat):\n",
    "    if item == '':\n",
    "        count+=1\n",
    "        bad_index.append(idx)\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for items in news_chat:\n",
    "    items=items\n",
    "    if not items:\n",
    "        continue\n",
    "    elif len(items)==1:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"argilla/news-summary\")\n",
    "article_key = 'text'\n",
    "summary_key = 'prediction'\n",
    "data = DatasetDict({\n",
    "    'train': data['test'],\n",
    "    'test': data['train']})\n",
    "\n",
    "data=data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.select(i for i in range(len(data)) \n",
    "                 if i not in set(bad_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_news(example):\n",
    "    example[\"original_article\"] = example[article_key]\n",
    "    example[\"article\"] = nltk.sent_tokenize(example[article_key])\n",
    "\n",
    "    check=example[summary_key][0]['text']\n",
    "    example[\"highlights\"] = nltk.sent_tokenize(check)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=48): 100%|███████████| 1000/1000 [00:00<00:00, 1947.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.map(tokenize_news, num_proc=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in data['highlights']:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in news_chat:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/reddit_capped_random.pkl', 'rb') as f:\n",
    "    reddit_chat=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486\n"
     ]
    }
   ],
   "source": [
    "#number of empty summaries\n",
    "count=0\n",
    "bad_index=[]\n",
    "for idx,item in enumerate(reddit_chat):\n",
    "    if item == []:\n",
    "        count+=1\n",
    "        bad_index.append(idx)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3728\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for items in reddit_chat:\n",
    "    #items=nltk.sent_tokenize(items)\n",
    "    if not items:\n",
    "        continue\n",
    "    elif len(items)==1:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('reddit_tifu', 'long')\n",
    "article_key = 'documents'\n",
    "summary_key = 'tldr'\n",
    "    # 80% train, 20% test + validation\n",
    "train_testvalid = data['train'].train_test_split(test_size=0.2, seed=42)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "data = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "data=data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.select(i for i in range(len(data)) \n",
    "                 if i not in set(bad_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=48): 100%|███████████| 3728/3728 [00:00<00:00, 6242.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.map(tokenize, num_proc=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5288\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in data['highlights']:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3728\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for summ in reddit_chat:\n",
    "    count+=len(summ)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
