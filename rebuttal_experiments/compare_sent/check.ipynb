{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haskari/miniconda3/envs/paraphrase/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/haskari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "import fire\n",
    "\n",
    "# from llama import Llama, Dialog\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']=\"1,2\"\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "import math, re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from transformers import Trainer, TrainingArguments, pipeline\n",
    "import argparse\n",
    "import evaluate\n",
    "# from styleformer import Styleformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import copy\n",
    "import multiprocessing\n",
    "import pickle as pkl\n",
    "import openai\n",
    "# from dotenv import load_dotenv\n",
    "import os\n",
    "from time import sleep\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Util functions\n",
    "def get_overlap_scores(sentences, document):\n",
    "    corpus = sentences + document\n",
    "    vect = TfidfVectorizer()\n",
    "    tfidf = vect.fit_transform(corpus)\n",
    "    similarities = (tfidf * tfidf.T).toarray()\n",
    "    \n",
    "    return similarities[:len(sentences), len(sentences):]\n",
    "\n",
    "\n",
    "def get_summary_indices(article, summary, top_k=1, tolerance=0.1):\n",
    "    scores = get_overlap_scores(summary, article)\n",
    "    idx = scores.argmax(axis=1)\n",
    "    false_idxs = np.where(scores.max(axis=1) == 0)\n",
    "    idx = np.delete(idx, false_idxs)\n",
    "    scores = np.delete(scores, false_idxs, axis=0)\n",
    "\n",
    "    if top_k > 1 and len(article) > 1:\n",
    "        search_idx = np.where((scores.max(axis=1) < 1-tolerance))\n",
    "        biggest_idx = np.argpartition(scores[search_idx], -top_k)[:, -top_k:]\n",
    "        unique_idx = np.concatenate((idx, biggest_idx.flatten()))\n",
    "        unique_idx = np.unique(unique_idx)\n",
    "    else:\n",
    "        unique_idx = np.unique(idx)\n",
    "    \n",
    "    unique_idx.sort()\n",
    "\n",
    "    return unique_idx\n",
    "\n",
    "\n",
    "def generate_n_segments(a, n=10): #NEW\n",
    "  k, m = divmod(len(a), n)\n",
    "  return list((i*k+min(i, m),(i+1)*k+min(i+1, m)) for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bertscore(original_sentences_final,paraphrased_sentences_final):\n",
    "    \n",
    "    highlights = []\n",
    "    model_s = []\n",
    "\n",
    "    for j,k in zip(original_sentences_final,paraphrased_sentences_final):\n",
    "        if not j or not k:\n",
    "            print('in skip')\n",
    "            continue\n",
    "        else:\n",
    "            highlights.append(' '.join(j))\n",
    "            model_s.append(' '.join(k))\n",
    "    \n",
    "    bertscore = load(\"bertscore\")\n",
    "    \n",
    "    results = bertscore.compute(predictions=model_s, references=highlights, lang=\"en\", device='cuda:1')\n",
    "    mean_precision=sum(results['precision'])/len(results['precision'])\n",
    "    mean_recall=sum(results['recall'])/len(results['recall'])\n",
    "    mean_f1=sum(results['f1'])/len(results['f1'])\n",
    "    \n",
    "    return mean_precision,mean_recall,mean_f1\n",
    "        \n",
    "def tokenize(example):\n",
    "    example[\"tokenized_document\"] = nltk.sent_tokenize(example[article_key])\n",
    "    example[\"tokenized_summary\"] = nltk.sent_tokenize(example[summary_key])\n",
    "   # example['segment_idxs'] = generate_n_segments(example[\"article\"]) #NEW\n",
    "    return example\n",
    "\n",
    "def tokenize_news(example):\n",
    "    example[\"tokenized_document\"] = nltk.sent_tokenize(example[article_key])\n",
    "    example[\"tokenized_summary\"] = nltk.sent_tokenize(example[summary_key][0][article_key])\n",
    "    #example['segment_idxs'] = generate_n_segments(example[\"article\"]) #NEW\n",
    "    return example\n",
    "\n",
    "\n",
    "def get_original_sentences(dataset, dataset_name,batch_size=1):\n",
    "    \n",
    "    if dataset_name!='news':\n",
    "        dataset = dataset.map(tokenize, num_proc=multiprocessing.cpu_count())\n",
    "    else:\n",
    "        dataset = dataset.map(tokenize_news, num_proc=multiprocessing.cpu_count())\n",
    "        \n",
    "    articles = dataset[\"tokenized_document\"]\n",
    "    highlights = dataset[\"tokenized_summary\"]\n",
    "\n",
    "    pp_articles = []\n",
    "\n",
    "    assert len(articles) == len(highlights), \"Error in dataset. Unequal lengths.\"\n",
    "    for i in tqdm(range(0, len(articles), batch_size)):\n",
    "        batch_articles = articles[i:min(i+batch_size, len(articles))]\n",
    "        batch_highlights = highlights[i:min(i+batch_size, len(articles))]\n",
    "        batch_pp_articles = []\n",
    "        batch_sentences = []\n",
    "        batch_idx = []\n",
    "        separator = 'X'\n",
    "        # collected=os.listdir(f\"../paraphrased_articles/{dataset_name}\")\n",
    "        \n",
    "        # if \"{}.pkl\".format(i) in collected:\n",
    "        #     continue\n",
    "\n",
    "        for j, (article, summ) in enumerate(zip(batch_articles, batch_highlights)):\n",
    "            \n",
    "            flag=False\n",
    "          \n",
    "            try:\n",
    "                idx = get_summary_indices(article, summ, top_k=2, tolerance=0.1)\n",
    "                # print(idx)\n",
    "            except Exception as e:\n",
    "                #print(\"in Except\")\n",
    "                #print(e)\n",
    "                pp_articles.append([])\n",
    "                break\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            original_sentences = [article[x] for x in idx]\n",
    "            # if (idx != 'A').all():\n",
    "            #     sentences = [article[x] for x in idx]\n",
    "            # else:\n",
    "            #     sentences = ['No']\n",
    "\n",
    "            batch_idx.extend(list(idx))\n",
    "            batch_idx.append(separator)\n",
    "            \n",
    "            batch_sentences.extend(original_sentences) \n",
    "            pp_articles.append(batch_sentences)\n",
    "\n",
    "    \n",
    "    return pp_articles\n",
    "\n",
    "\n",
    "\n",
    "def get_paraphrased_sentences(dataset, dataset_name,batch_size=1):\n",
    "    paraphrased_articles=[]\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        with open('../../paraphrased_articles/{}/{}.pkl'.format(dataset_name,i), 'rb') as f:\n",
    "            untokenized_article=pkl.load(f)\n",
    "            paraphrased_articles.append(\" \".join(untokenized_article))\n",
    "    \n",
    "    dataset = dataset.remove_columns(article_key)\n",
    "    dataset = dataset.add_column(article_key, paraphrased_articles)\n",
    "    \n",
    "    print(dataset[article_key][0])\n",
    "    \n",
    "    paraphrased_sent=get_original_sentences(dataset,dataset_name)\n",
    "    \n",
    "    return paraphrased_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=128): 100%|██████████| 1000/1000 [00:00<00:00, 1352.83 examples/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 78389.41it/s]\n",
      "Map (num_proc=128): 100%|██████████| 1000/1000 [00:04<00:00, 237.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "article_key = 'article'\n",
    "summary_key = 'highlights'\n",
    "name='cnn_dailymail'\n",
    "dataset=dataset['test']\n",
    "\n",
    "dataset=dataset.select(range(1000))\n",
    "first_unpara=dataset[article_key][0]\n",
    "name='cnn'\n",
    "\n",
    "dataset_original = dataset.map(tokenize, num_proc=multiprocessing.cpu_count())\n",
    "dataset_original_articles=dataset_original[\"tokenized_document\"]\n",
    "\n",
    "\n",
    "paraphrased_articles=[]\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    with open('../../paraphrased_articles/{}/{}.pkl'.format('cnn',i), 'rb') as f:\n",
    "        untokenized_article=pkl.load(f)\n",
    "        paraphrased_articles.append(\" \".join(untokenized_article))\n",
    "\n",
    "dataset = dataset.remove_columns(article_key)\n",
    "dataset = dataset.add_column(article_key, paraphrased_articles)\n",
    "# first_para=dataset[article_key][0]\n",
    "# print(get_bertscore(first_unpara,first_para))\n",
    "dataset=dataset.select(range(1000))\n",
    "# print(dataset)\n",
    "# print(dataset['highlights'])\n",
    "\n",
    "dataset_paraphrased = dataset.map(tokenize, num_proc=multiprocessing.cpu_count())\n",
    "dataset_paraphrased_articles=dataset_paraphrased[\"tokenized_document\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraphrase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
