{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "# from typing import List, Optional\n",
    "\n",
    "# import fire\n",
    "\n",
    "# from llama import Llama, Dialog\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "# from time import sleep\n",
    "# from datasets import load_dataset, DatasetDict\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import pickle as pkl\n",
    "# import os\n",
    "\n",
    "# # os.environ['CUDA_VISIBLE_DEVICES']=\"1,2\"\n",
    "# import pandas as pd\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "# import torch\n",
    "# from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# from nltk import sent_tokenize\n",
    "# import math, re\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# from torchmetrics.text.rouge import ROUGEScore\n",
    "# from transformers import Trainer, TrainingArguments, pipeline\n",
    "# import argparse\n",
    "# import evaluate\n",
    "# # from styleformer import Styleformer\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# import copy\n",
    "# import multiprocessing\n",
    "# import pickle as pkl\n",
    "# import openai\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reddit/0.pkl\", 'rb') as f:\n",
    "    file=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well this was around 9 years ago when i was a 2nd grader. so basically it was some kids birthday and since it was primary school most kids brought cupcakes and the teacher would pass it out to the class. me being the hungry little shit that i was i was desperately in the mood for some vanilla cupcakes but the teacher decided to pass out the chocolate ones first. after she had finished passing out the chocolate ones she picks up the vanilla ones and says, \"raise your hand if you want vanilla!\". at that moment i was talking to a friend and it took me a bit to process the fact that she just asked who wanted vanilla. I reached up and found the cupcakes were conveniently located just above me. I accidentally knocked the cupcakes out of her hand, causing them to spill all over her. she got bat-shit angry and begins to make a scene in the middle of the whole cafeteria. she explained how i \"ruined her brand new shoes\" and the bitch even asked me if i had an allowance so i could pay for her shoes. she had asked me at the time if i was messing around and i said yes because i literally didn\\'t know what else to say. after numerous emails from my mom to the teacher things finally cleared up and all was back to normal. **']\n"
     ]
    }
   ],
   "source": [
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('reddit_tifu', 'long')\n",
    "article_key = 'documents'\n",
    "summary_key = 'tldr'\n",
    "# 80% train, 20% test + validation\n",
    "train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "name='reddit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4214"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well this was around 9 years ago when i was a 2nd grader. so basically it was some kids birthday and since it was primary school most kids brought cupcakes and the teacher would pass it out to the class. me being the hungry little shit that i was i was desperately in the mood for some vanilla cupcakes but the teacher decided to pass out the chocolate ones first. \\n\\nafter she had finished passing out the chocolate ones she picks up the vanilla ones and says, \"raise your hand if you want vanilla!\". at that moment i was talking to a friend and it took me a bit to process the fact that she just asked who wanted vanilla. i proceed to raise my hand and it turns out the cupcakes were right above me. i knocked the cupcakes out of her hand and they spilled  all over her. she got bat-shit angry and begins to make a scene in the middle of the whole cafeteria. she explained how i \"ruined her brand new shoes\" and the bitch even asked me if i had an allowance so i could pay for her shoes. \\n\\nshe had asked me at the time if i was messing around and i said yes because i literally didn\\'t know what else to say. after numerous emails from my mom to the teacher things finally cleared up and all was back to normal.\\n\\n**'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'][0]['paraphrased']='lol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_articles=['lol']*4214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(\"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.add_column(\"documents\", paraphrased_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    example[\"tokenized_document\"] = nltk.sent_tokenize(example[article_key])\n",
    "    example[\"tokenized_summary\"] = nltk.sent_tokenize(example[summary_key])\n",
    "   # example['segment_idxs'] = generate_n_segments(example[\"article\"]) #NEW\n",
    "    return example\n",
    "\n",
    "def tokenize_news(example):\n",
    "    example[\"tokenized_document\"] = nltk.sent_tokenize(example[article_key])\n",
    "    example[\"tokenized_summary\"] = nltk.sent_tokenize(example[summary_key][0][article_key])\n",
    "    #example['segment_idxs'] = generate_n_segments(example[\"article\"]) #NEW\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paraphrased_articles(generator,dataset, dataset_name, max_gen_len,temperature,top_p,batch_size=1):\n",
    "    \n",
    "    if dataset_name!='news':\n",
    "        dataset = dataset.map(tokenize, num_proc=multiprocessing.cpu_count())\n",
    "    else:\n",
    "        dataset = dataset.map(tokenize_news, num_proc=multiprocessing.cpu_count())\n",
    "        \n",
    "    articles = dataset[\"tokenized_document\"]\n",
    "    highlights = dataset[\"tokenized_summary\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
