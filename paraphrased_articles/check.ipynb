{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haskari/miniconda3/envs/acl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/haskari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-23 20:39:24,261] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "from typing import List, Optional\n",
    "\n",
    "import fire\n",
    "\n",
    "from llama import Llama, Dialog\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']=\"1,2\"\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "import math, re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from transformers import Trainer, TrainingArguments, pipeline\n",
    "import argparse\n",
    "import evaluate\n",
    "# from styleformer import Styleformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import copy\n",
    "import multiprocessing\n",
    "import pickle as pkl\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"xsum/0.pkl\", 'rb') as f:\n",
    "    file=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation. Workers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders. The Welsh Government said more people than ever were getting help to address housing problems.  Prison Link Cymru, which helps people find accommodation after their release, said things were generally good for women because issues such as children or domestic violence were now considered. However, the same could not be said for men, the charity said, because issues which often affect them, such as post traumatic stress disorder or drug dependency, were often viewed as less of a priority.  \"There\\'s a desperate need for it, finding suitable accommodation for those leaving prison there is just a lack of it everywhere,\" he said. \"It could take six months to a year, without a lot of help they could be on the streets for six months. \"When you think of the consequences of either being on the street, especially with the cold weather at the moment or you may have a roof over your head, sometimes there is only one choice.\" Mr Stevens believes building more one-bedroom flats could help ease the problem. \"The average price is a hundred pounds a week to keep someone in a rented flat, prison is a lot more than that so I would imagine it would save the public purse quite a few pounds,\" he said. Official figures show 830 one-bedroom properties were built in the year to March 2016, of an overall total of 6,900 new properties in Wales. Marc, 50, who has been in and out of prison for the past 20 years for burglary offences, said he struggled to find accommodation each time he was released. He said he would ask himself: \"Where am I going to stay? Where am I going to live? Have I got somewhere where I can see my daughter.\" \"You\\'re put out among the same sort of people doing the same sort of thing, and it\\'s difficult, it\\'s difficult to get away from it. It\\'s like every man for himself, there\\'s nothing.\" Marc has now found stable accommodation with homeless charity Emmaus and said it had been life changing. \"You feel safe, you got hot food, you\\'ve got company of people in similar situations to yourself but all dealing with different issues. It\\'s a constructive, helpful atmosphere,\" he said. Tom Clarke, chief executive of Emmaus South Wales, agreed there was not enough support available. \"We do still see [people] homeless on the streets, so clearly they haven\\'t got accommodation and haven\\'t got provision,\" he said. \"I think the key is connecting people with the services they need. I don\\'t delude myself that Emmaus can offer a one size fits all for everyone, we can\\'t. \"But there must be other opportunities and given suitable encouragement I believe that can and should happen.\" A Welsh Government spokesman said the national pathway for homeless services to children, young people and adults in the secure estate had prevented many people from losing their home whilst serving their prison sentence. It added there were already significant demands for one-bedroom flats across the public and private sector and it was providing 20,000 new affordable homes in the next five years.']\n"
     ]
    }
   ],
   "source": [
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('reddit_tifu', 'long')\n",
    "article_key = 'documents'\n",
    "summary_key = 'tldr'\n",
    "# 80% train, 20% test + validation\n",
    "train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "name='reddit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4214"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well this was around 9 years ago when i was a 2nd grader. so basically it was some kids birthday and since it was primary school most kids brought cupcakes and the teacher would pass it out to the class. me being the hungry little shit that i was i was desperately in the mood for some vanilla cupcakes but the teacher decided to pass out the chocolate ones first. \\n\\nafter she had finished passing out the chocolate ones she picks up the vanilla ones and says, \"raise your hand if you want vanilla!\". at that moment i was talking to a friend and it took me a bit to process the fact that she just asked who wanted vanilla. i proceed to raise my hand and it turns out the cupcakes were right above me. i knocked the cupcakes out of her hand and they spilled  all over her. she got bat-shit angry and begins to make a scene in the middle of the whole cafeteria. she explained how i \"ruined her brand new shoes\" and the bitch even asked me if i had an allowance so i could pay for her shoes. \\n\\nshe had asked me at the time if i was messing around and i said yes because i literally didn\\'t know what else to say. after numerous emails from my mom to the teacher things finally cleared up and all was back to normal.\\n\\n**'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'][0]['paraphrased']='lol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_articles=['lol']*4214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(\"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.add_column(\"documents\", paraphrased_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    example[\"tokenized_document\"] = nltk.sent_tokenize(example[article_key])\n",
    "    example[\"tokenized_summary\"] = nltk.sent_tokenize(example[summary_key])\n",
    "   # example['segment_idxs'] = generate_n_segments(example[\"article\"]) #NEW\n",
    "    return example\n",
    "\n",
    "def tokenize_news(example):\n",
    "    example[\"tokenized_document\"] = nltk.sent_tokenize(example[article_key])\n",
    "    example[\"tokenized_summary\"] = nltk.sent_tokenize(example[summary_key][0][article_key])\n",
    "    #example['segment_idxs'] = generate_n_segments(example[\"article\"]) #NEW\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paraphrased_articles(generator,dataset, dataset_name, max_gen_len,temperature,top_p,batch_size=1):\n",
    "    \n",
    "    if dataset_name!='news':\n",
    "        dataset = dataset.map(tokenize, num_proc=multiprocessing.cpu_count())\n",
    "    else:\n",
    "        dataset = dataset.map(tokenize_news, num_proc=multiprocessing.cpu_count())\n",
    "        \n",
    "    articles = dataset[\"tokenized_document\"]\n",
    "    highlights = dataset[\"tokenized_summary\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
